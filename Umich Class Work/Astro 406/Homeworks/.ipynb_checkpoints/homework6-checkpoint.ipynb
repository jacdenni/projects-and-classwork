{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astronomy 406 \"Computational Astrophysics\" (Fall 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 6 (due Thursday, Nov 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [<b>emcee</b>](http://dfm.io/emcee) MCMC ensemble sampler to map the posterior distribution of a 2D Gaussian with strong correlation between the two parameters.\n",
    "The likelihood is given by\n",
    "$$\n",
    "    {\\cal L}(p_1,p_2) = {1 \\over 2\\pi \\sigma_1 \\sigma_2 \\sqrt{1-r^2}}\n",
    "    \\exp{\\left( - {1\\over 2(1-r^2)} \\left[ \n",
    "      \\frac{(p_1-\\mu_1)^2}{\\sigma_1^2} + \\frac{(p_2-\\mu_2)^2}{\\sigma_2^2}\n",
    "      -\\frac{2r(p_1-\\mu_1)(p_2-\\mu_2)}{\\sigma_1\\sigma_2}\n",
    "    \\right]\\right)}\n",
    "$$\n",
    "and we take $\\mu_1 = 1$, $\\mu_2 = 5$, $\\sigma_1 = 1$, $\\sigma_2 = 0.2$, $r = 0.9$.\n",
    "\n",
    "Determine how many steps to burn and how long to run the chain, using the Gelman-Rubin convergence indicator.  You can use the routines below, from the Week 10 notebook. Individual chains in emcee are 3D arrays, which can be accessed as <tt>sampler.chain[walker,step,parameter]</tt>.\n",
    "\n",
    "The autocorrelation time calculation should succeed without errors if you apply it only after the chain has grown to about 1000 steps for each walker.  I also set an internal parameter that controls the minimum number of autocorrelation times needed to trust the estimate to 3 instead of default 10: <tt>tacor = sampler.get_autocorr_time(c=3)</tt>.\n",
    "\n",
    "1. Write the <tt>lnLikelihood</tt> and <tt>lnPosterior</tt> functions for the above distribution, choosing appropriately wide priors.\n",
    "\n",
    "2. Start the emcee chains in random locations and extend them iteratively in chunks of $\\sim 100$ steps of each walker. Choose at least 100 walkers. Check the Gelman-Rubin convergence indicator after each iteration and stop when the difference $|R_{GR}-1|$ falls below 1%.  Do not discard any burn-in samples yet or reset the chain.\n",
    "\n",
    "3. Store the values of $R_{GR}-1$ and the autocorrelation times for both parameters after each iteration, and plot them (vs. step number) after the chains converge.\n",
    "\n",
    "4. Calculate the mean and standard deviation of the two parameters from the full chain, and the correlation coefficient between the parameters.\n",
    "\n",
    "5. Plot the parameter distributions from the flattened chain (combined for all walkers) using the top-hat KDE, with superimposed Gaussian distribution with the calculated mean and standard deviation.  Is there a good correspondence?  Whether your anser is yes or no, what do you think is the reason for it?\n",
    "\n",
    "6. Use the <tt>plot_samples</tt> routine to plot the parameter distribution from the full chain.\n",
    "\n",
    "7. Use the calculated auto-correlation time to determine how many steps to <b>burn</b> and how to <b>thin</b> the remaining samples. You need to burn the first steps of each walker chain, which you can do as\n",
    "<tt>p1t=[]; [p1t.append(sampler.chain[i,nburn::nthin,0]) for i in range(nwalkers)]; p1t=np.array(p1t).flatten()</tt>\n",
    "Here nburn is the number of steps to burn for each walker.  Use the <tt>plot_samples</tt> routine for the thinned chain.  How does it differ from the full chain?\n",
    "\n",
    "8. Recalculate the mean and standard deviation of the two parameters from the thinned chain, and compare with the target values ($\\mu_k$ and $\\sigma_k$).  Same for the correlation coefficient.  Do they agree better than the full chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_samples(xh, yh):\n",
    "    \"\"\"\n",
    "    This routine plots the 2D histogram of MCMC samples and their contour levels,\n",
    "      along with the contours of the target distribution.\n",
    "    It uses the same user-supplied 'lnLikelihood' function used to run the emcee sampler,\n",
    "      'mu' is the array of mean target parameters.\n",
    "    Arguments are flattened 1D arrays of two samples.\n",
    "    \"\"\"\n",
    "    plt.xlabel(r'$p_1$')\n",
    "    plt.ylabel(r'$p_2$')\n",
    "    \n",
    "    # 2D color histogram\n",
    "    plt.hist2d(xh, yh, bins=100, norm=LogNorm(), normed=1)\n",
    "    plt.colorbar()\n",
    "                      \n",
    "    # target likelihood level contours\n",
    "    x = np.arange(np.min(xh)-1, np.max(xh)+1, 0.05)\n",
    "    y = np.arange(np.min(yh)-1, np.max(yh)+1, 0.05)      \n",
    "    X, Y = np.meshgrid(x,y)  \n",
    "    Z = [np.exp(lnLikelihood([x1,x2])) for x1,x2 in zip(X,Y)]\n",
    "    \n",
    "    # contours enclosing 68.27 and 99% of the probability density\n",
    "    dlnL2 = np.array([9.21, 2.30])\n",
    "    lnLmax = lnLikelihood(mu)\n",
    "    lvls = np.exp(lnLmax-0.5*dlnL2)\n",
    "    cs = plt.contour(X,Y,Z, linewidths=1.5, colors='black', norm = LogNorm(), levels = lvls)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GR_indicator(mwx, swx, mwy, swy, nchain):\n",
    "    \"\"\"\n",
    "    Gelman-Rubin convergence indicator.\n",
    "    Arguments are 1D arrays of sums of two sample values (mwx,mwy) and their squares (swx,swy), \n",
    "      nchain is the number of elements in each walker chain.\n",
    "    \"\"\"\n",
    "    mwxc = mwx/(nchain-1.);  mwyc = mwy/(nchain-1.) \n",
    "    swxc = swx/(nchain-1.)-np.power(mwxc,2)\n",
    "    swyc = swy/(nchain-1.)-np.power(mwyc,2)\n",
    "    # within chain variance\n",
    "    Wgrx = np.sum(swxc)/nwalkers; Wgry = np.sum(swyc)/nwalkers\n",
    "    # mean of the means over Nwalkers\n",
    "    mx = np.sum(mwxc)/nwalkers; my = np.sum(mwyc)/nwalkers\n",
    "    # between chain variance\n",
    "    Bgrx = nchain*np.sum(np.power(mwxc-mx,2))/(nwalkers-1.)\n",
    "    Bgry = nchain*np.sum(np.power(mwyc-my,2))/(nwalkers-1.)\n",
    "        \n",
    "    # Gelman-Rubin R factor\n",
    "    Rgrx = (1 - 1/nchain + Bgrx/Wgrx/nchain)*(nwalkers+1)/nwalkers - (nchain-1)/(nchain*nwalkers)\n",
    "    Rgry = (1 - 1/nchain + Bgry/Wgry/nchain)*(nwalkers+1)/nwalkers - (nchain-1)/(nchain*nwalkers)\n",
    "\n",
    "    return np.array([ Rgrx, Rgry ])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
