{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astronomy 406 \"Computational Astrophysics\" (Fall 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 10: Markov Chain Monte Carlo and Monte Carlo integration (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reading:</b> notes below, as well as [$\\S$15.8](https://umich.instructure.com/files/655099/download), [$\\S$7.7-7.9](https://umich.instructure.com/files/715726/download) of [Numerical Recipes](http://numerical.recipes/), and $\\S$5.8 of [Machine Learning](http://www.astroml.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 90\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kde_tophat( data, x, h ):\n",
    "    y = (abs(x - data[:,None]) <= h).astype(float)\n",
    "    return y.sum(0)/(2*h*len(data))\n",
    "\n",
    "def pdf_gauss( x, mu, s ):\n",
    "    return np.exp(-((x-mu)/s)**2/2)/(np.sqrt(2*np.pi)*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lgal, dLgal, Mgc, dMgc = np.loadtxt(\"gc_galaxy.dat\", unpack=True)\n",
    "\n",
    "ind = (Lgal > 9) \n",
    "x = Lgal[ind]; dx = dLgal[ind]; y = Mgc[ind]; dy = dMgc[ind];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c8e735b8d074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnburn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$\\sigma$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "import corner\n",
    "corner.corner(chain[nburn:n], labels=['b', '$\\sigma$', 'a'], bin=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of use of MCMC for a Gaussian Mixture is given in <a href=\"http://www.astroml.org/book_figures/chapter5/fig_model_comparison_mcmc.html\">Figure 5.24</a> in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient implementation of MCMC ensemble sampler is the [<b>emcee</b>](http://dan.iel.fm/emcee/current/) package.  Here is how one could use for our example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convergence indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two general types of convergence criteria have been proposed in the literature: testing stationarity of the chain parameter distribution ([Gelman & Rubin 1992](https://projecteuclid.org/euclid.ss/1177011136), [Brooks & Gelman 1998](https://stat.duke.edu/~scs/Courses/Stat376/Papers/ConvergeDiagnostics/BrooksGelman.pdf)), or measuring the auto-correlation length of the chain samples  compared to the total length of the chain ([Dunkley 2005](http://adsabs.harvard.edu/abs/2005MNRAS.356..925D), [Goodman & Weare 2010](http://msp.org/camcos/2010/5-1/p04.xhtml)).\n",
    "\n",
    "<b>The Gelman-Rubin indicator</b>\n",
    "\n",
    "The idea behind this indicator is that if we have a number of individually advanced chains (\"walkers\"), we can compare the variance we get within each chain to the variance we get among different chains. Perfect convergence would correspond to the \"within chain\" and \"between chain\" values of the variance matching each other. Thus, the Gelman-Rubin indicator is defined as the ratio of the two variances:\n",
    "$$\n",
    "R=\\frac{V}{W}=\\frac{N_w+1}{N_w}\\frac{\\sigma^2_+}{W}-\\frac{N-1}{N_w \\, N},\n",
    "$$\n",
    "where  $N_w$ is the number of walkers (independent chains) and $N$ is the total length of each of the individual chains,  \n",
    "$$\n",
    "\\sigma^2_+=\\frac{N-1}{N}W + \\frac{B}{N};\n",
    "$$\n",
    "if we denote the random variable vector $\\vec{x}$ and denote $\\vec{x}_{jt}$ the $t$th of the $N$ steps of the chain $j$ \"between-walker\" variance is\n",
    "$$\n",
    "\\frac{B}{N}=\\frac{1}{N_w-1}\\sum\\limits_{j=1}^{N_w}(\\bar{\\vec{x}}_{j.}-\\bar{\\vec{x}}_{..})^2\n",
    "$$\n",
    "and \"within chain\" variance $W$ is \n",
    "$$\n",
    "W=\\frac{1}{N_w(N-1)}\\sum\\limits_{j=1}^{N_w}\\sum\\limits_{t=1}^{N}(\\vec{x}_{jt}-\\vec{x}_{j.})^2\n",
    "$$\n",
    "\n",
    "Examples below show the convergence indicator $R_{\\rm GR}\\equiv R-1.0$ calculated after every $1000^{\\rm th}$ step as a function of step in the chains run with the [Goodman & Weare (2010)](http://msp.org/camcos/2010/5-1/p04.xhtml) ensemble sampling algorithm for the 2D Gaussian and Rosenbrock pdf using a given number $N_w={\\tt nwalkers}$ independent chains. The chains for the Gaussian pdf converge relatively fast. However, the difficulty of the Rosenbrock \"banana\" pdf will be apparent in the slow convergence of $R_{\\rm GR}$ to zero.\n",
    "\n",
    "<b>Indicators based on the auto-correlation time</b>\n",
    "\n",
    "The second commonly used measure of convergence is the <i>auto-correlation time.</i> By design, MCMC samples are correlated over a certain number of steps, often quite strongly. Such correlations can bias estimates of the variance of the parameters and thus one needs to know the frequency of the truly independent samples. Moreover, it is the number of independent samples that actually matters for the accuracy of the quantities estimated from the MCMC chains, not the total number of generated samples.  The auto-correlation time defined below is a natural quantity to measure the spacing between samples that can be considered independent. It can be used to come up with criteria to estimate the <i>burn-in</i> length of improbable samples at the beginning of the chain. It can further be used to figure out how many independent samples the chains have and what is the corresponding accuracy of the estimated parameters that can then be used to come up with criteria for stopping the chains, as described below.  \n",
    "\n",
    "For some real valued function of the parameter vector $f=f(\\mathbf{x})$. Now consider the stationary Markov chain (i.e.\n",
    "start the system in the stationary distribution $\\pi(\\mathbf{x})$, or equivalently, equilibrate it for a\n",
    "very long time prior to observing the system). Then $\\{f\\}\\equiv \\left\\{f(\\mathbf{x}_t)\\right\\}$ is a stationary\n",
    "stochastic process with mean\n",
    "\n",
    "$$\n",
    "\\mu_f\\equiv \\langle f\\rangle=\\sum\\limits_{j}\\pi(\\mathbf{x}_j)f(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Note that function $f$ here is whatever quantity or function that you are interested in to get from the MCMC chain. For example, if you are interested in the parameters themselves: $f(\\mathbf{x}_j)=\\mathbf{x}_j$ - i.e, in the 2D pdfs considered below, $f_1=x_1$ and $f_2=x_2$ (you estimate auto-correlation time for the two parameters separately). If for some reason you are interested instead in $x_1^2+x^2_2$, you should use $f=x_1^2+x^2_2$ in the computation of the correlation function. \n",
    "\n",
    "The unnormalized auto-correlation function of the chain is \n",
    "$$C_{ff}(t)=\\langle f_sf_{s+t}\\rangle-\\mu_f^2,$$\n",
    "while the normalized function is\n",
    "$$\\rho_{ff}(t)\\equiv C_{ff}(t)/C_{ff}(0).$$ The auto-correlation time can be measured directly from $\\rho_{ff}$. However, typically $\\rho_{ff}$ can be approximated as an exponential $\\rho_{ff}(t)\\approx \\exp^{-t/\\tau}$ so that the <i>exponential auto-correlation time can be defined</i> for large $t\\rightarrow\\infty$:\n",
    "$$\\tau_{exp,f}=\\max\\left(\\frac{t}{-\\log\\vert\\rho_{ff}\\vert}\\right)$$\n",
    "The overall autocorrelation time is then\n",
    "$$\n",
    "\\tau_{exp}=\\max_f\\tau_{exp,f},\n",
    "$$\n",
    "i.e., the auto-correlation time is equilibration time of the slowest mode in the system.\n",
    "\n",
    "We will be using the <tt>acor</tt> python wrapper for the C++ routines of Goodman written by Dan Foreman-Mackey. You can get it by installing in your python distro using <tt>pip install acor</tt>, or get it [here](https://pypi.python.org/pypi/acor/1.1.1) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an estimate of $R_{\\rm GR}$ and $\\tau_{\\rm exp}\\,$ we need criteria for how to decide on convergence. \n",
    "\n",
    "$R_{\\rm GR}$ should be steadily approaching unity near equilibrium, for example $R_{\\rm GR} < 1.01$.\n",
    "\n",
    "Given that we will estimate quantities of interest from the Monte Carlo samples of the posterior, the fractional accuracy of the estimate will be $\\approx 1/\\sqrt{N}$, where $N$ is the number of <i>independent</i> samples. That is $N$ is not the total number of samples we have generated in the MCMC chains, $N_s$, but only a fraction of them that can be considered independent. Auto-correlation time $\\tau_{\\rm exp}$ gives us the number of iterations spacing independent samples and thus the fractional error of estimating a parameter from MCMC will be\n",
    "$$\\epsilon\\approx\\left(\\frac{\\tau_{\\rm exp}}{N_s-N_{\\rm b}}\\right)^{1/2},$$\n",
    "where $N_s$ is the total number of samples in the chain after $N_{\\rm b}$ burn-in samples are removed. Thus, to reach $1\\%$ accuracy in a given parameter one needs a chain of length $N_s\\approx 10^4\\tau_{\\rm exp}+N_{\\rm b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GR_indicator(mwx, swx, mwy, swy, nchain):\n",
    "    mwxc = mwx/(nchain-1.);  mwyc = mwy/(nchain-1.) \n",
    "    swxc = swx/(nchain-1.)-np.power(mwxc,2)\n",
    "    swyc = swy/(nchain-1.)-np.power(mwyc,2)\n",
    "    # within chain variance\n",
    "    Wgrx = np.sum(swxc)/nwalkers; Wgry = np.sum(swyc)/nwalkers\n",
    "    # mean of the means over Nwalkers\n",
    "    mx = np.sum(mwxc)/nwalkers; my = np.sum(mwyc)/nwalkers\n",
    "    # between chain variance\n",
    "    Bgrx = nchain*np.sum(np.power(mwxc-mx,2))/(nwalkers-1.)\n",
    "    Bgry = nchain*np.sum(np.power(mwyc-my,2))/(nwalkers-1.)\n",
    "        \n",
    "    # Gelman-Rubin R factor\n",
    "    Rgrx = (1 - 1/nchain + Bgrx/Wgrx/nchain)*(nwalkers+1)/nwalkers - (nchain-1)/(nchain*nwalkers)\n",
    "    Rgry = (1 - 1/nchain + Bgry/Wgry/nchain)*(nwalkers+1)/nwalkers - (nchain-1)/(nchain*nwalkers)\n",
    "\n",
    "    return Rgrx, Rgry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn-in and thinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start the chain near the peak of the posterior. However, often information about the posterior is limited, at least for some of the parameters of the problem. The initial guess can thus be quite a bit off in the low-probability region. If steps are chosen reasonably, the chain will recover and, in fact, the initial samples in the low probability region are formally correct samples of the target pdf. \n",
    "Nevertheless,  these low probability values are often extremely improbable for the finite length of the sample chain that one generates in practice.  For example, if the 1D Gaussian pdf of zero mean and unit variance, $N(0,1)$, is sampled with the chain that was initialized at $x_0=10$ -- i.e., $10\\sigma$ away from the peak, the <a href=\"http://www.aleph.se/andart/archives/2009/09/ten_sigma_numerics_and_finance.html\">probability</a> of such sample is $\\approx 1.5\\times 10^{-23}$. Thus, we would need to have a chain of length $N\\sim 10^{23}$ to make such sample normal. For samples of smaller $N$ this starting value can bias estimates of the mean, rms dispersion, etc. For example, with such initialization it takes the Metropolis algorithm $\\sim 50-100$ steps to recover to the probably range of values. Thus, the first $\\sim 50-100$ samples will need to be discarded for any realistic chain length to avoid any biases. \n",
    "\n",
    "The initial range of the improbable samples is called the burn-in period of the chain. Determining this period is handled in conjunction with deciding on chain convergence. Simple checks by how much one's estimates of the statistics of interest change after discarding a certain number of the initial samples may be enough in many instances, but a better criterion is to use auto-correlation time to determine the length of burn-in.  <a href=\"http://www.stat.unc.edu/faculty/cji/Sokal.pdf\">Sokal (1996)</a> recommends to discard the first $N_b=20\\tau_{\\rm exp}$ samples, where $\\tau_{\\rm exp}$ is the auto-correlation time described above. This is implemented in the code above, although it may be overly conservative.\n",
    "\n",
    "As noted above, MCMC chains produce $\\sim N/\\tau$ effectively independent samples. The chains thus are often <i>\"thinned\"</i> by selecting only each $(1\\tau)^{th}$ sample and discarding the rest that don't contribute anything to the accuracy of the estimates from the posterior.  \n",
    "\n",
    "The above criteria for burn-in and thinning are implemented in the above code. The walkers in the test above were initialized at $x=10$, $y=0$ - well away from the peak of the pdf. You can see that the burn-in criterion got rid of the improbable initial samples quite well. You can see them if you set ${\\tt nburn}=0$. Overall, the sampling worked correctly as can be seen by good agreement between the 68% contour from the samples (magenta line) and expected for this pdf (thin black solid line under magenta). \n",
    "\n",
    "You can also see that the sampler generated $N_s\\sim 6\\times 10^5$ samples, but what's actually plotted is a rather noisy pdf sampling because the plotted chains were thinned and only $N\\sim 10^4$ independent samples are plotted ($\\tau\\approx 50$). You can see that $R_{\\rm GR}=1.01$ convergence criterion resulted in the number of independent samples sufficient to make estimates from the posterior with $\\approx 1\\%$ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise with a 2D Gaussian function\n",
    "\n",
    "Let's do a simple test with a 2D Gaussian with non-zero correlation - a fairly typical posterior for two well-constrained parameters.\n",
    "\n",
    "The likelihood is given by\n",
    "$$\n",
    "    {\\cal L}(p_1,p_2) = {1 \\over 2\\pi \\sigma_1 \\sigma_2 \\sqrt{1-r^2}}\n",
    "    \\exp{\\left( - {1\\over 2(1-r^2)} \\left[ \n",
    "      \\frac{p_1^2}{\\sigma_1^2} + \\frac{p_2^2}{\\sigma_2^2}\n",
    "      -\\frac{2r p_1 p_2}{\\sigma_1\\sigma_2}\n",
    "    \\right]\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2D Gaussian function with correlation between variables\n",
    "def lnLikelihood(p):\n",
    "    sig1 = 1.; sig2 = 1.; r = 0.95; r2 = r*r\n",
    "    x1 = p[0]\n",
    "    x2 = p[1]\n",
    "    twologL = -((x1/sig1)**2 + (x2/sig2)**2 - 2*r*x1*x2/(sig1*sig2))/(1-r2)\\\n",
    "                -2.*np.log(2*np.pi*sig1*sig2/np.sqrt(1-r2))\n",
    "    return twologL/2.\n",
    "\n",
    "def lnPrior(p):\n",
    "    if not (-10 < p[0] < 10):\n",
    "        return -np.inf\n",
    "    if not (-10 < p[1] < 10):\n",
    "        return -np.inf\n",
    "    return 0.0\n",
    "\n",
    "def lnPosterior(p):\n",
    "    return lnPrior(p) + lnLikelihood(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean acceptance fraction = 0.669\n",
      "p1 = -0.012 +- 1.160  p2 = 0.943 +- 0.679\n"
     ]
    }
   ],
   "source": [
    "import emcee\n",
    "\n",
    "ndim = 3\n",
    "nwalkers = 100\n",
    "nsteps = 300000\n",
    "nburn = 20000\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose an initial set of positions for the walkers\n",
    "p0 = [np.random.rand(ndim) for i in xrange(nwalkers)]\n",
    "\n",
    "# Initialize the sampler with the chosen specs\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnPosterior)\n",
    "\n",
    "# Run burn-in steps\n",
    "pos, prob, state = sampler.run_mcmc(p0, nburn/nwalkers)\n",
    "\n",
    "# Reset the chain to remove the burn-in samples\n",
    "sampler.reset()\n",
    "\n",
    "# Starting from the final position in the burn-in chain, sample nsteps\n",
    "sampler.run_mcmc(pos, nsteps/nwalkers, rstate0=state)\n",
    "\n",
    "# Print out the mean acceptance fraction among all walkers\n",
    "print \"Mean acceptance fraction = %.3f\" % np.mean(sampler.acceptance_fraction)\n",
    "\n",
    "# Estimate the integrated autocorrelation time for the time series in each parameter.\n",
    "#print \"Autocorrelation length =\", sampler.get_autocorr_time(low = 5, c = 1)\n",
    "\n",
    "print \"p1 = {0:.3f} +- {1:.3f}  p2 = {2:.3f} +- {3:.3f}\"\\\n",
    "  .format(np.mean(sampler.flatchain[:,0]), np.std(sampler.flatchain[:,0]), \\\n",
    "   np.mean(np.abs(sampler.flatchain[:,1])), np.std(np.abs(sampler.flatchain[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up getting very wrong values. Why do these appear? Because of our really rather random nsteps and nburn. We can fix this with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 0.035 +- 1.002  p2 = 0.036 +- 1.004 nchain = 500.0\n",
      "p1 = 0.019 +- 0.996  p2 = 0.019 +- 0.997 nchain = 1000.0\n",
      "p1 = 0.007 +- 1.000  p2 = 0.005 +- 1.001 nchain = 1500.0\n",
      "p1 = 0.010 +- 1.002  p2 = 0.007 +- 1.003 nchain = 2000.0\n",
      "p1 = 0.007 +- 1.000  p2 = 0.003 +- 1.000 nchain = 2500.0\n",
      "p1 = 0.007 +- 1.000  p2 = 0.003 +- 1.001 nchain = 2800.0\n",
      "Correlation Coefficients: 0.950527936902\n"
     ]
    }
   ],
   "source": [
    "ndim = 2\n",
    "nwalkers = 100\n",
    "nsteps = 10000\n",
    "\n",
    "#Choose an initial set of positions for the walkers\n",
    "pos = [np.random.rand(ndim) for i in xrange(nwalkers)]\n",
    "\n",
    "#Initialize the sampler with the chosen specs\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnPosterior)\n",
    "\n",
    "nthin = 35\n",
    "nburn = int(20 * nwalkers * nthin)\n",
    "\n",
    "#Run burn-in steps\n",
    "pos, prob, state = sampler.run_mcmc(pos, nburn / nwalkers)\n",
    "\n",
    "#Reset the chain to remove the burn-in samples\n",
    "sampler.reset()\n",
    "\n",
    "converged = False\n",
    "\n",
    "nchain = 0.\n",
    "numLoops = 0\n",
    "while not converged:\n",
    "    numLoops += 1\n",
    "    pos, prob, state = sampler.run_mcmc(pos, nburn / nwalkers, rstate0 = state)\n",
    "    mwx = np.array([np.sum(sampler.chain[i, :, 0]) for i in range(nwalkers)])\n",
    "    mwy = np.array([np.sum(sampler.chain[i, :, 1]) for i in range(nwalkers)])\n",
    "    swx = np.array([np.sum(sampler.chain[i, :, 0]**2) for i in range(nwalkers)])\n",
    "    swy = np.array([np.sum(sampler.chain[i, :, 1]**2) for i in range(nwalkers)])\n",
    "    \n",
    "    nchain += nsteps/nwalkers\n",
    "    Rx, Ry = GR_indicator(mwx, swx, mwy, swy, nchain)\n",
    "\n",
    "    if (np.fabs(Rx - 1.) < 0.01) and (np.fabs(Ry - 1.) < .01):\n",
    "        converged = True\n",
    "    if (numLoops % 5) == 0:\n",
    "        #print 'Running... numLoops =', numLoops\n",
    "        print \"p1 = {0:.3f} +- {1:.3f}  p2 = {2:.3f} +- {3:.3f} nchain = {4}\"\\\n",
    "          .format(np.mean(sampler.flatchain[::nthin,0]), np.std(sampler.flatchain[::nthin,0]), \\\n",
    "           np.mean(sampler.flatchain[::nthin,1]), np.std(sampler.flatchain[::nthin,1]), nchain)\n",
    "print \"p1 = {0:.3f} +- {1:.3f}  p2 = {2:.3f} +- {3:.3f} nchain = {4}\"\\\n",
    "    .format(np.mean(sampler.flatchain[::nthin,0]), np.std(sampler.flatchain[::nthin,0]), \\\n",
    "    np.mean(sampler.flatchain[::nthin,1]), np.std(sampler.flatchain[::nthin,1]), nchain)    \n",
    "print 'Correlation Coefficients:', stats.pearsonr(sampler.flatchain[::nthin, 0], \n",
    "                                                  sampler.flatchain[::nthin, 1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  73.08554398 -560.45663149  207.79222173 -600.48752907 -542.30245541\n",
      "  -62.43486079 -132.43265491  213.86883707  -87.1936809  -228.45442882\n",
      "  -42.14634824    2.90092328   87.0915663  -254.11311477 -126.86702643\n",
      "  296.5467365  -304.55433336 -565.0969939  -112.22786883 -159.21336951\n",
      " -130.02454403 -286.34659658 -119.71339661    1.59279011 -306.84109762\n",
      " -298.90417655  -59.10737639 -155.99940687 -175.27142281 -740.96854683\n",
      "  -43.10025984  -16.60889195  -45.51877675  157.24244008 -123.65249937\n",
      "  -75.74157123 -115.53911413  -73.11920721  309.65544827  111.55573846\n",
      " -240.29571897   11.90120912  293.66909588 -361.76698111  -85.6861837\n",
      "   28.63559476 -533.02960208   12.40968436   83.5394863  -150.11628826\n",
      "  194.86608645 -106.95401218  150.20569855    9.09409727  -58.48463709\n",
      "  102.78812385  -81.95199117    8.48404481  596.13142051  240.6444292\n",
      " -188.52987065  205.83675232 -223.47690848   86.32516377  451.75387812\n",
      " -354.09449739  259.16546244 -427.10927542  -39.3421335   279.61409107\n",
      "  109.12856745 -102.88713971   48.08885471  -87.31167034   37.88115435\n",
      " -360.63578121 -142.65791446  212.66941245  -17.35155728  313.17397446\n",
      "  164.62831653   14.43352363  513.31555846 -286.22786847  -95.36826739\n",
      " -309.90120163  -18.60155603  103.14702533   20.3497318    28.411588\n",
      " -216.5793018  -225.94981142  323.26656243  294.1405363  -392.01397031\n",
      "  -41.40678183   55.50902245   63.79873605 -485.47928388   74.74991711]\n"
     ]
    }
   ],
   "source": [
    "# access full chain for one parameter\n",
    "p1 = zip(*sampler.flatchain)[0]\n",
    "\n",
    "nthin = int(max(sampler.get_autocorr_time()))   # about 50\n",
    "nburn = int(20*nwalkers*nthin)\n",
    "\n",
    "# calculate mean value of chain for individual walkers, and put them into numpy array\n",
    "mwx = np.array([np.sum(sampler.chain[i,:,0]) for i in range(nwalkers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick $N$ random points $x_i$, uniformly distributed in a multidimensional volume $V$. The basic theorem of Monte Carlo integration estimates the integral of a function $f$ over the multidimensional volume as\n",
    "\n",
    "$$ \\int f(\\mathbf{x}) dV \\approx V \\langle f\\rangle \\pm V \\sqrt{\\langle f^2\\rangle - \\langle f\\rangle^2 \\over N}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\langle f\\rangle \\equiv {1\\over N} \\sum\\limits_{i=0}^{N-1} f(x_i). $$\n",
    "\n",
    "It is implemented in a simple routine below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_mcintegration(integrand, xmin, xmax, ndim, n):\n",
    "    f, f2 = 0., 0.\n",
    "    x = np.zeros(ndim)\n",
    "    # integration volume\n",
    "    volume = 1.\n",
    "    for j in range(ndim):\n",
    "        volume *= (xmax[j]-xmin[j])\n",
    "    # calculate mean f and f^2\n",
    "    for i in range(n):\n",
    "        for j in range(ndim):\n",
    "            x[j] = np.random.uniform(xmin[j], xmax[j])\n",
    "        s = integrand(x)\n",
    "        f += s\n",
    "        f2 += s**2\n",
    "    f = f/n\n",
    "    f2 = f2/n\n",
    "    # calculate the value of integral and estimated error\n",
    "    result = volume*f\n",
    "    error = volume*np.sqrt((f2-f**2)/n)\n",
    "    return result, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a very simple example - integration of the volume of a 3D sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =    1000  Int = 4.1680  estimated error = 0.1264  actual error = 0.0208\n",
      "n =   10000  Int = 4.2496  estimated error = 0.0399  actual error = 0.0608\n",
      "n =  100000  Int = 4.1834  estimated error = 0.0126  actual error = 0.0054\n",
      "n = 1000000  Int = 4.1856  estimated error = 0.0040  actual error = 0.0032\n"
     ]
    }
   ],
   "source": [
    "def integrand(x):\n",
    "    r = x[0]**2 + x[1]**2 + x[2]**2\n",
    "    if r <= 1.:\n",
    "        #return 3./4./np.pi\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "ndim = 3\n",
    "xmin = -1.*np.ones(ndim)\n",
    "xmax = 1.*np.ones(ndim)\n",
    "expected = 4.*np.pi / 3.\n",
    "\n",
    "for n in [1000, 10000, 100000, 1000000]:\n",
    "    result, error = simple_mcintegration(integrand, xmin, xmax, ndim, n)\n",
    "    \n",
    "    print \"n = %7d  Int = %.4f  estimated error = %.4f  actual error = %.4f\"\\\n",
    "        % (n, result, error, abs(result-expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a more difficult case - integration over a thin shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =    1000  Int = 1.9118  estimated error = 1.1021  actual error = 0.9118\n",
      "n =   10000  Int = 1.0833  estimated error = 0.2625  actual error = 0.0833\n",
      "n =  100000  Int = 1.0770  estimated error = 0.0828  actual error = 0.0770\n",
      "n = 1000000  Int = 1.0062  estimated error = 0.0253  actual error = 0.0062\n"
     ]
    }
   ],
   "source": [
    "def integrand(x):\n",
    "    r2 = x[0]**2 + x[1]**2 + x[2]**2\n",
    "    d = 0.999\n",
    "    d2 = d**2\n",
    "    if d2 <= r2 and r2 <= 1.:\n",
    "        return 3./4./np.pi/(1-d**3)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "for n in [1000, 10000, 100000, 1000000]:\n",
    "    result, error = simple_mcintegration(integrand, xmin, xmax, ndim, n)\n",
    "    \n",
    "    print \"n = %7d  Int = %.4f  estimated error = %.4f  actual error = %.4f\"\\\n",
    "        % (n, result, error, abs(result-expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration over a spherical volume can be done more efficiently in spherical coordinates:\n",
    "\n",
    "$$ \\int dV = \\int\\limits_{0}^{r} dr \\int\\limits_{0}^{\\pi} d\\theta \\int\\limits_{0}^{2\\pi} d\\phi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Exercise 2:</b> change integration variables to spherical coordinates and compare the integration accuracy with the previous case of Cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =    1000  Int = 0.5504  estimated error = 0.1352  actual error = 0.4496\n",
      "n =   10000  Int = 0.5631  estimated error = 0.0445  actual error = 0.4369\n",
      "n =  100000  Int = 0.4919  estimated error = 0.0128  actual error = 0.5081\n",
      "n = 1000000  Int = 0.4999  estimated error = 0.0041  actual error = 0.5001\n"
     ]
    }
   ],
   "source": [
    "def sphere_integrand(r):\n",
    "    rho = r[0]; theta = r[1]; phi = r[2];\n",
    "    if (rho <= 1.) and (theta <= np.pi) and (phi <= (2. * np.pi)):\n",
    "        return 3. / (4. * np.pi) * rho**2 * np.sin(theta)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def sphere_slice_integrand(r):\n",
    "    rho = r[0]; theta = r[1]; phi = r[2];\n",
    "    if (rho <= 1.) and (theta <= np.pi * 2./3.) and (np.pi / 3 <= theta) \\\n",
    "    and (phi <= (2. * np.pi)):\n",
    "        return 3. / (4. * np.pi) * rho**2 * np.sin(theta)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "ndim = 3\n",
    "rmin = np.array([0., 0., 0.])\n",
    "rmax = np.array([1.5, 1.5 * np.pi, 3. * np.pi])\n",
    "expected = 1.\n",
    "\n",
    "for n in [1000, 10000, 100000, 1000000]:\n",
    "    result, error = simple_mcintegration(sphere_slice_integrand, xmin, xmax, ndim, n)\n",
    "    print \"n = %7d  Int = %.4f  estimated error = %.4f  actual error = %.4f\"\\\n",
    "        % (n, result, error, abs(result-expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read on more efficient sampling techniques in [$\\S$7.9](https://umich.instructure.com/files/715726/download) of [Numerical Recipes](http://numerical.recipes/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
